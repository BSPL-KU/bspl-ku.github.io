<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/interesting_works/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/interesting_works/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/interesting_works/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/interesting_works/"><meta property="og:title" content="Interesting Works | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-04-26T00:00:00+00:00"><title>Interesting Works | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=6968ecbcb5e980dded5666bc95f80948><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a></div></li><li class=nav-item><a class="nav-link active" href=/interesting_works><span>Interesting Works</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Interesting Works</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2023_04_26_zhang_et_al_cvpr2022/>[Article] Transformer-based multimodal information fusion for facial expression analysis.</a></div><div class=mt-2><p>summary : In this work, they utilize multimodal features of spoken words, speech prosody and facial expression from Aff-WIld2 dataset. They combine these features using a transformer-based fusion module which makes the output embedding features of sequences of images, audio and text. Integrated output feature is then processed in MLP layer for Action Unit (AU) detection and also facial expression recognition.</p><p><a href=https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Zhang_Transformer-Based_Multimodal_Information_Fusion_for_Facial_Expression_Analysis_CVPRW_2022_paper.pdf target=_blank rel=noopener>Zhang, Wei, et al. &ldquo;Transformer-based multimodal information fusion for facial expression analysis.&rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 26, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2023_04_26_quzar_et_al_cvpr2022/>[Article] Video-based multimodal spontaneous emotion recognition using facial expressions and physiological signals.</a></div><div class=mt-2><p>summary : In this work, they propose the first video-based multimodal spontaneous emotion recognition that combines facial expressions with physiological data to derive the advantages of each modality. The feature vector of facial expression is fused with physiological signals including iPPG signal and HRV. The feature-level fusioned input is then processed in a 3D Xception-net based DNN model.</p><p><a href=https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Ouzar_Video-Based_Multimodal_Spontaneous_Emotion_Recognition_Using_Facial_Expressions_and_Physiological_CVPRW_2022_paper.pdf target=_blank rel=noopener>Ouzar, Yassine, et al. &ldquo;Video-based multimodal spontaneous emotion recognition using facial expressions and physiological signals.&rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 26, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2023_04_05_nassar_et_al_jneuro/>[Article] OFC represents shifting latent states in learning.</a></div><div class=mt-2><p>summary : This paper investigated how the neural representations for learning would change during rapid behavioral changes. Various cortical regions contribute to the representational changes, notably DLPFC and ACC representing uncertainty, and OFC for representations of (rapidly) shifting contexts.</p><p><a href=https://www.jneurosci.org/content/39/9/1688 target=_blank rel=noopener>Nassar, M. R., McGuire, J. T., Ritz, H., & Kable, J. W. (2019). Dissociable forms of uncertainty-driven representational change across the human brain. Journal of Neuroscience, 39(9), 1688-1698.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 5, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2023_03_22_murphy_et_al_nature_communication/>[Article] Multimodal network dynamics underpinning working memory.</a></div><div class=mt-2><p>Summary: First, they found WM performance is related to FPN and DMN coupling. Furthermore, they found two sub-networks of FPN and showed how their activity, FC, and SC are related to the integrative processing of complex cognition using HCP 2-back task.</p><p><a href=https://www.nature.com/articles/s41467-020-15541-0 target=_blank rel=noopener>Murphy, A. C., Bertolero, M. A., Papadopoulos, L., Lydon-Staley, D. M., & Bassett, D. S. (2020). Multimodal network dynamics underpinning working memory. Nature communications, 11(1), 3035.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 22, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2023_03_08_wang_et_al_nature_communication/>[Article] scGNN is a novel graph neural network framework for single-cell RNA-Seq analyses.</a></div><div class=mt-2><p>summary : This paper introduced single-cell graph neural network (scGNN) to provide a hypothesis-free deep learning framework for scRNA-Seq analysis. This formulates and aggregates cell-cell relationships with GNN and models heterogeneous gene expressio naptterns using a left-truncated mixture Gaussian model. They integrates three iterative multi-modal autoencoders and outperforms existing tools for gene imputation and cell clustering.</p><p><a href=https://www.nature.com/articles/s41467-021-22197-x target=_blank rel=noopener>Wang, Juexin, et al. &ldquo;scGNN is a novel graph neural network framework for single-cell RNA-Seq analyses.&rdquo; Nature communications 12.1 (2021): 1882.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 8, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2022_12_28_zhang_liangfei_et_al_ieee_affective_computing/>[Article] Short and long range relation based spatio-temporal transformer for micro-expression recognition.</a></div><div class=mt-2><p>Summary: In this paper, they investigate facial micro-expression that is getting much attention recently. They propose a novel spatio-temporal transformer architecture - the first purely transformer based approach for micro-expression recognition. It captures both local and global spatio-temporal patterns of video in an end-to-end way. This model is currently SOTA in MER(Micro-Expression Recognition) task.</p><p><a href=https://bspl.korea.ac.kr/Board/Members_Only/Lab_Seminar/JJS/SLSST.pdf target=_blank rel=noopener>Zhang, Liangfei, et al. &ldquo;Short and long range relation based spatio-temporal transformer for micro-expression recognition.&rdquo; IEEE Transactions on Affective Computing 13.4 (2022).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Dec 28, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2022_12_21_mo_shentong_et_al_arxiv/>[Article] Multi-modal Self-supervised Pre-training for Regulatory Genome Across Cell Types.</a></div><div class=mt-2><p>Summary: Current deep learning methods often focus on modeling genome sequences of a fixed set of cell types and do not account for the interaction between multiple regulatory elements. They propose a simple yet effective approach for pre-training genome data in a multi-modal and self-supervised manner, which we call GeneBERT. They pre-train and evaluate GeneBERT model on regulatory downstream tasks across different cell types, including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction.</p><p><a href=https://bspl.korea.ac.kr/Board/Members_Only/Lab_Seminar/JSH/GeneBERT.pdf target=_blank rel=noopener>Mo, Shentong, et al. &ldquo;Multi-modal Self-supervised Pre-training for Regulatory Genome Across Cell Types.&rdquo; arXiv preprint arXiv:2110.05231 (2021).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Dec 21, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2022_11_30_mahmood_et_al_neuroimage/>[Article] Through the looking glass: deep interpretable dynamic directed connectivity in resting fMRI</a></div><div class=mt-2><p>Summary: Static functional connectivity matrix is usually calculated using simple Pearson&rsquo;s correlation coefficients. This is simple, but cannot represent the dynamic relations of our brain. Here, they applied self-attention to calculate the attention scores of each embedded region and temporal attention to compute the weighted sum of these dynamic functional connectivities. Using this architecture, called DICE, they were able to classify mental disorders, genders, and predict age in different big datasets.</p><p><a href=https://bspl.korea.ac.kr/Board/Members_Only/Lab_Seminar/HJD/Mahmood%282022%29_22nov30.pdf target=_blank rel=noopener>Mahmood, U., Fu, Z., Ghosh, S., Calhoun, V., & Plis, S. (2022). Through the looking glass: deep interpretable dynamic directed connectivity in resting fMRI. NeuroImage, 119737.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Nov 30, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2022_11_16_zhao_et_al_neuroimage/>[Article] A dynamic graph convolutional neural network framework reveals new insights into connectome dysfunctions in ADHD.</a></div><div class=mt-2><p>summary : They proposed novel approach to incorporate dynamic graph computation and 2-hop neighbor nodes feature aggregation into graph convolution for brain network modeling. They used convolutional pooling strategy to readout the graph, which jointly integrates graph convolutional and readout functions. They could visualize model weights which showed interpretable connectomic patterns facilitating the understanding of brain functional abnormalities.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1053811921010466 target=_blank rel=noopener>Zhao, Kanhao, et al. &ldquo;A dynamic graph convolutional neural network framework reveals new insights into connectome dysfunctions in ADHD.&rdquo; Neuroimage 246 (2022): 118774.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Nov 16, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/interesting_works/2022_11_02_yan_et_al_cvpr2022/>[Article] Multiview transformers for video recognition.</a></div><div class=mt-2><p>Summary: This is follow-up study of Video Vision Transformer. For multiscale modelling in video recognition task, they used multiview tubulets and applied cross-view attention over seperate transformer models. It is currently SOTA in kinetics600 and 5 other standard video benchmarks. It was introduced in CVPR2022. They also have official code in the scenic project.</p><p><a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.pdf target=_blank rel=noopener>Yan, Shen, et al. &ldquo;Multiview transformers for video recognition.&rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Nov 2, 2022</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/interesting_works/page/2/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2023 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script>
<script src=/en/js/wowchemy.min.2cc80485e7b9001edba5cdf5b39a1f97.js></script></body></html>