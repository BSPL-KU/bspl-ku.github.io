<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/seminar_papers/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/seminar_papers/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/seminar_papers/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/seminar_papers/"><meta property="og:title" content="Seminar Papers | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-08-12T00:00:00+00:00"><title>Seminar Papers | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1ef0c7c24519ec8ae8f7b9f6b71a9a5f><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a>
<a class=dropdown-item href=https://github.com/bsplku><span>GitHub</span></a></div></li><li class=nav-item><a class="nav-link active" href=/seminar_papers><span>Seminar Papers</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Seminar Papers</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_08_12_li_et_al_nature_communications/>[Article] Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation</a></div><div class=mt-2><p>Summary: This paper presents a comprehensive framework for generating radiology reports from 3D brain CT scans using a multimodal large language model
<a href=https://www.nature.com/articles/s41467-025-57426-0 target=_blank rel=noopener>Li, Cheng-Yi, et al. &ldquo;Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation.&rdquo; Nature Communications 16.1 (2025): 2258.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Aug 12, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_07_29_ma_et_al_cell_reports_damme_et_al_translational_psychiatry_de_moraes_jaha/>[Article] Neural correlates of device-based sleep characteristics in adolescents & Physical and mental health in adolescence: novel insights from a transdiagnostic examination of Fitbit data in the ABCD study & Impact of Environmental Noise and Sleep Health on Pediatric Hypertension Incidence: ABCD Study</a></div><div class=mt-2><h3 id=neural-correlates-of-device-based-sleep-characteristics-in-adolescents>Neural correlates of device-based sleep characteristics in adolescents</h3><p>Summary: This study integrated wearable device-measured sleep characteristics (sleep onset time, sleep duration, heart rate, etc.) with multimodal brain imaging data using sCCA in 3,222 adolescents participating in the ABCD Study to identify two major sleep-brain axes: one in which late sleep onset and short sleep duration were associated with reduced cortical-subcortical connectivity, and another in which high sleep heart rate and short light sleep duration were associated with reduced brain volume and connectivity.</p><p><a href="https://www.cell.com/cell-reports/fulltext/S2211-1247%2825%2900336-5?uuid=uuid%3Abbfc6e94-7ed3-4127-b490-c95c9a9db92a" target=_blank rel=noopener>Ma, Qing, et al. &ldquo;Neural correlates of device-based sleep characteristics in adolescents.&rdquo; Cell Reports 44.5 (2025).</a></p><h3 id=physical-and-mental-health-in-adolescence-novel-insights-from-a-transdiagnostic-examination-of-fitbit-data-in-the-abcd-study>Physical and mental health in adolescence: novel insights from a transdiagnostic examination of Fitbit data in the ABCD study</h3><p>Summary: This study analyzed cross-sectionally how resting heart rate, sedentary time, and moderate activity time measured by Fitbit wearables were associated with pleiotropic-like experiences (PLEs), internalizing, and externalizing symptoms in 5,007 adolescents aged 10–13 years from the ABCD cohort.</p><p><a href=https://www.nature.com/articles/s41398-024-02794-2 target=_blank rel=noopener>Damme, Katherine SF, et al. &ldquo;Physical and mental health in adolescence: novel insights from a transdiagnostic examination of FitBit data in the ABCD study.&rdquo; Translational psychiatry 14.1 (2024): 75.</a></p><h3 id=impact-of-environmental-noise-and-sleep-health-on-pediatric-hypertension-incidence-abcd-study>Impact of Environmental Noise and Sleep Health on Pediatric Hypertension Incidence: ABCD Study</h3><p>Summary: This study analyzed data from 3320 participants of the ABCD study. Hypertension was defined as average blood pressure >=95th percentile for age, sex and height. They revealed that adequate sleep significantly reduces the risk of hypertension in adolescents, independent of environmental noise exposure. These findings underscore the importance of promoting good sleep hygiene among youth to mitigate hypertension risk.</p><p><a href=https://www.ahajournals.org/doi/full/10.1161/JAHA.124.037503 target=_blank rel=noopener>De Moraes, Augusto César F., et al. &ldquo;Impact of Environmental Noise and Sleep Health on Pediatric Hypertension Incidence: ABCD Study.&rdquo; Journal of the American Heart Association 13.22 (2024): e037503.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jul 29, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_07_15_soroka_et_al_current_biology/>[Article] Humans have nasal respiratory fingerprints</a></div><div class=mt-2><p>Summary: This study shows that long-term nasal airflow patterns are unique to each individual, stable over months to years, and can identify people with near-biometric accuracy. These “respiratory fingerprints” also reflect physiological states, such as sleep and BMI, as well as psychological traits like depression, anxiety, and autistic tendencies. The findings highlight nasal airflow as a rich, brain-driven signal linking physiology, emotion, and cognition.
<a href=https://www.cell.com/current-biology/fulltext/S0960-9822%2825%2900583-4 target=_blank rel=noopener>Soroka, Timna, et al. &ldquo;Humans have nasal respiratory fingerprints.&rdquo; Current Biology (2025).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jul 15, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_06_24_zaccaro_et-_al_iscience/>[Article] Attention to cardiac sensations enhances the heartbeat-evoked potential during exhalation</a></div><div class=mt-2><p>Summary: They recently found higher HEP amplitude during exhalation than during inhalation during a task that involved attention to cardiac sensations. This may have been due to reduced cardiac perception during inhalation and heightened perception during exhalation, mediated by attentional mechanisms. To investigate relationships between HEP, attention, and respiration, they introduced an experimental setup that included tasks related to cardiac and respiratory interoceptive and exteroceptive attention. Results revealed HEP amplitude increases during the interoceptive tasks over fronto-central electrodes. When respiratory phases were taken into account, HEP increases were primarily driven by heartbeats recorded during exhalation, specifically during the cardiac interoceptive task, while inhalation had minimal impact.
<a href=https://www.sciencedirect.com/science/article/pii/S2589004224008083?via%3Dihub target=_blank rel=noopener>Zaccaro, Andrea, et al. &ldquo;Attention to cardiac sensations enhances the heartbeat-evoked potential during exhalation.&rdquo; Iscience 27.4 (2024).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jun 24, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_06_10_setzer_et_al_nature_communications/>[Article] A temporal sequence of thalamic activity unfolds at transitions in behavioral arousal state</a></div><div class=mt-2><p>Summary: This study measured sub-second activity within thalamocortical networks and nine thalamic nuclei in the human brain during spontaneous transitions in behavioral arousal state, using ultra-high field fast fMRI. The research discovered a stereotyped sequence of activity across thalamic nuclei and the cingulate cortex that preceded behavioral arousal after a period of inactivity, followed by widespread deactivation. These thalamic dynamics were linked to whether participants subsequently fell back into unresponsiveness, with unified thalamic activation reflecting the maintenance of behavior.
<a href=https://www.nature.com/articles/s41467-022-33010-8#citeas target=_blank rel=noopener>Setzer, B., Fultz, N.E., Gomez, D.E.P. et al. A temporal sequence of thalamic activity unfolds at transitions in behavioral arousal state. Nat Commun 13, 5442 (2022).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jun 10, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_05_20_qiu_et_al_arxiv/>[Article] MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding</a></div><div class=mt-2><p>Summary: In this seminar, we will explore a novel fMRI-to-text decoding framework named MindLLM. It combines a neuroscience-informed, subject-agnostic fMRI encoder with an off-the-shelf large language model to translate brain activity into coherent text. It introduces Brain Instruction Tuning (BIT), which enriches the model’s capacity to extract and represent diverse semantic information from fMRI signals, enabling versatile decoding across different tasks and subjects. We will discuss how to implement these techniques in our study.
<a href=https://arxiv.org/abs/2502.15786 target=_blank rel=noopener>Qiu, Weikang, et al. &ldquo;MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding.&rdquo; arXiv preprint arXiv:2502.15786 (2025).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 20, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_04_29_koush_et_al_neuroimage/>[Article] OpenNFT: An open-source Python/Matlab framework for real-time fMRI neurofeedback training based on activity, connectivity and multivariate pattern analysis</a></div><div class=mt-2><p>Summary: In this seminar, we will explore the structure of an open-source rtfMRI-NF toolbox. They use Python and MATLAB to preprocess data, generate feedback, and provide it.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1053811917305050 target=_blank rel=noopener>Koush, Yury, et al. &ldquo;OpenNFT: An open-source Python/Matlab framework for real-time fMRI neurofeedback training based on activity, connectivity and multivariate pattern analysis.&rdquo; NeuroImage 156 (2017): 489-503.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 29, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_04_15_wang_et_al_arxiv/>[Article] Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models</a></div><div class=mt-2><p>Summary: In this study, to enhance our understanding of visual processes, they developed WAVE, which reconstructs visual stimuli from fMRI data. By integrating three modalities (fMRI, image, and text) to perform contrastive learning, the features are then passed to a diffusion model for final image reconstruction.</p><p><a href=https://arxiv.org/abs/2411.07121 target=_blank rel=noopener>Wang, Turnbull, et al. &ldquo;Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models.&rdquo; arXiv(2024)</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 15, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_03_25_liu_et_al_cell/>[Article] Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations</a></div><div class=mt-2><p>Summary: The study performed traditional binary GWAS, continuous univariate GWAS using wearable combination scores, and multivariate GWAS to identify genetic variants associated with ADHD. The identified variants showed associations with heart function (MYH6, CMTM5) and ADHD-related genes (ELFN1), with some variants potentially having a protective effect against ADHD.</p><p><a href=https://www.cell.com/cell/fulltext/S0092-8674%2824%2901329-1 target=_blank rel=noopener>Liu, Jason J., et al. &ldquo;Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations.&rdquo; Cell (2024).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 25, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_03_12_thapaliya_et_al_medical_image_analysis/>[Article] Brain networks and intelligence: A graph neural network based approach to resting state fmri data</a></div><div class=mt-2><p>Summary: This study introduces BrainRGIN, a novel graph neural network (GNN) model designed to predict intelligence using resting-state fMRI data. By leveraging graph isomorphism networks and clustering-based embeddings, the model effectively captures brain sub-network structures. The authors validate their approach using the Adolescent Brain Cognitive Development Dataset and demonstrate superior predictive performance compared to traditional machine learning models.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S136184152400358X target=_blank rel=noopener>Thapaliya, Bishal, et al. &ldquo;Brain networks and intelligence: A graph neural network based approach to resting state fmri data.&rdquo; Medical Image Analysis 101 (2025): 103433.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 12, 2025</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/seminar_papers/page/2/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2025 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e762603fd04b1d81f1e953bafed73e89.js></script></body></html>