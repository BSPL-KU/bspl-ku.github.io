<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/seminar_papers/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/seminar_papers/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/seminar_papers/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/seminar_papers/"><meta property="og:title" content="Seminar Papers | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-09-09T00:00:00+00:00"><title>Seminar Papers | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1ef0c7c24519ec8ae8f7b9f6b71a9a5f><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a>
<a class=dropdown-item href=https://github.com/bsplku><span>GitHub</span></a></div></li><li class=nav-item><a class="nav-link active" href=/seminar_papers><span>Seminar Papers</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Seminar Papers</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_08_29_cruz_et_al_ieee/>[Article] Eye-lrcn: A long-term recurrent convolutional network for eye blink completeness detection. IEEE Transactions on Neural Networks and Learning Systems.</a></div><div class=mt-2><p>Summary: The article introduces Eye-LRCN, a new method for eye blink detection that also evaluates blink completeness using a Long-Term Recurrent Convolutional Network (LRCN). This approach combines a CNN for feature extraction with a bidirectional RNN for sequence learning, and employs a Siamese architecture to handle class imbalance and limited data. Eye-LRCN demonstrates superior performance in blink detection and completeness assessment, and achieves noticeable results in eye state detection.</p><p><a href=https://ieeexplore.ieee.org/abstract/document/9885029 target=_blank rel=noopener>de la Cruz, Gonzalo, et al. &ldquo;Eye-lrcn: A long-term recurrent convolutional network for eye blink completeness detection.&rdquo; IEEE Transactions on Neural Networks and Learning Systems 35.4 (2022): 5130-5140.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Aug 29, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_08_14_menon_neuron/>[Article] 20 years of the default mode network: A review and synthesis. Neuron.</a></div><div class=mt-2><p>Summary: The author thoroughly reviewed organization of the default mode network (DMN) and cognitive roles of the DMN (i.e., self-reference, social cognition, memory, mind wandering). Finally, he suggested a new perspective of the DMN function in human cognitition, in which the DMN intergrate and &ldquo;broadcast&rdquo; various representations to create coherent &ldquo;interal narrative&rdquo;.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S0896627323003082 target=_blank rel=noopener>Menon, V. (2023). 20 years of the default mode network: A review and synthesis. Neuron.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Aug 14, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_07_31_kumar_et_al_nature_communication/>[Article] Shared functional specialization in transformer-based language models and the human brain.</a></div><div class=mt-2><p>Summary: Transformers are recently being compared to the brain. Usually, the internal representations (“embeddings”) are adopted for comparisons. However, the authors focused on “transformations” that integrate contextual information across words, and found that they are more layer-specific than the embeddings. It differs from existing research in that it focuses on transformations related to attention instead of embeddings, which has been one of our recent interests.</p><p><a href=https://www.nature.com/articles/s41467-024-49173-5 target=_blank rel=noopener>Kumar, S., Sumers, T. R., Yamakoshi, T., Goldstein, A., Hasson, U., Norman, K. A., &mldr; & Nastase, S. A. (2024). Shared functional specialization in transformer-based language models and the human brain. Nature Communications, 15(1), 5523.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jul 31, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_07_17_jihunkim_et_al/>[not published] 1. Protocol to investigate the strategic manipulation of human causal inference through in-silico task design 2. Improving the Adaptivity of Reinforcement Learning Agent Based on the Prefrontal Cortex Meta-Control Theory of the Human Brain</a></div><div class=mt-2><p>summary : This study introduces a cognitive model and task controller to enhance human causal inference abilities through controlled learning strategies, including one-shot and incremental learning. It aims to optimize the efficiency of learning causal relationships by manipulating the presentation sequence of stimulus-outcome pairs, with potential applications in cognitive training.<br>summary : This thesis investigates methods to enhance the adaptivity of reinforcement learning agents based on the prefrontal cortex meta-control theory of the human brain. The proposed Meta-Dyna algorithm is designed to adapt flexibly to changes in the environment and has demonstrated optimal performance in various settings.</p><p><a href=https://bspl.korea.ac.kr/Board/Members_Only/Lab_Seminar/KJH/KJH_labseminar_24Jul17.pdf target=_blank rel=noopener>KJH_lab_seminar_24Jul17.pdf</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jul 17, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_06_19_caro_et_al_biorxiv/>[Article] BrainLM: A foundation model for brain activity recordings</a></div><div class=mt-2><h3 id=brainlm-a-foundation-model-for-brain-activity-recordings>BrainLM: A foundation model for brain activity recordings</h3><p>Summary: This paper suggested BrainLM which is a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings.</p><p><a href=https://www.biorxiv.org/content/10.1101/2023.09.12.557460v2.full target=_blank rel=noopener>Ortega Caro, Josue, et al. &ldquo;BrainLM: A foundation model for brain activity recordings.&rdquo; bioRxiv (2023): 2023-09.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jun 19, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_06_17_xiew_et_al_nature_medicine/>[Article] A shared neural basis underlying psychiatric comorbidity.</a></div><div class=mt-2><p>Summary: Utilizing large longitudinal neuroimaging cohort (from adolescence to young adulthood) (IMAGEN), they use multitask connectomes to find neuropsychopathological (NP) factor. They also check generalizability of the NP factor with ABCD (and other) datasets.</p><p><a href=https://www.nature.com/articles/s41591-023-02317-4 target=_blank rel=noopener>Xie, C., Xiang, S., Shen, C., Peng, X., Kang, J., Li, Y., &mldr; & ZIB Consortium. (2023). A shared neural basis underlying psychiatric comorbidity. Nature medicine, 29(5), 1232-1242.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jun 17, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_06_05_chaiebleila_et_al_frontiers_in_psychiatry/>[Article] Auditory beat stimulation and its effects on cognition and mood states.</a></div><div class=mt-2><p>Summary: This paper presents a comprehensive review of auditory beat stimulation, with a particular focus on the applications and features of binaural beats. Despite the extensive research conducted on binaural beats, there is still a lack of consensus regarding the consistent effects and the underlying neural mechanisms.</p><p><a href=https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2015.00070/full target=_blank rel=noopener>Chaieb, Leila, et al. &ldquo;Auditory beat stimulation and its effects on cognition and mood states.&rdquo; Frontiers in psychiatry 6 (2015): 136819.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jun 5, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_05_22_justin-hudak_et_al_neuropsychopharmacology/>[Article] Endogenous theta stimulation during meditation predicts reduced opioid dosing following treatment with Mindfulness-Oriented Recovery Enhancement.</a></div><div class=mt-2><p>Summary: This study investigates the effectiveness of the Mindfulness-Oriented Recovery Enhancement (MORE) program for police opioid users. Experimental results showed that participants exhibited an increase in theta and alpha brain waves during mindfulness meditation, along with improved coherence of mid-frontal theta. These neurophysiological changes were associated with reductions in opioid use, related to self-transcendence induced by mindfulness.</p><p><a href=https://www.nature.com/articles/s41386-020-00831-4 target=_blank rel=noopener>Hudak, J., Hanley, A.W., Marchand, W.R. et al. Endogenous theta stimulation during meditation predicts reduced opioid dosing following treatment with Mindfulness-Oriented Recovery Enhancement. Neuropsychopharmacol. 46, 836–843 (2021)</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 22, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_05_01_liu_et_al_pattern_recognition/>[Article] Expression snippet transformer for robust video-based facial expression recognition</a></div><div class=mt-2><p>Summary: Although Transformer can be powerful for modeling visual relations and describing complicated patterns, it could still perform unsatisfactorily for video-based facial expression recognition, since the expression movements in a video can be too small to reflect meaningful spatial-temporal relations. They propose to decompose the modeling of expression movements of a video into the modeling of a series of expression snippets, each of which contains a few frames. Their propsed model, Expression Snippet Transformer (EST) process intra-snippet and inter-snippet information seperately and combine them together. Code is available in github.</p><p><a href="https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320323X00028/1-s2.0-S0031320323000699/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDt2P6bMhy6Lgb1O9LxiNbyAx4sDh3VBo4a8Ubo4B9ebQIhAP9cmOfvLTtJ7aOxbNWclPNHqqAmPCDjxR1u6ZY5CN%2B3KrsFCJz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igzig%2FFM%2FVlxcsnj5iAqjwW%2Ffco0MPow3tza2ZmvoQu6kgeWn%2F1ipwiNsjqCtFyPqGESVobiKyK2qhL%2BO6gLjuzAZpS5%2BbPsWbRTDZw2TGqFXJ%2BUm3E%2BAbpk6%2FcTLSUJ%2BhcpY212lG9MGzZ455TkmAluQ2crUyVs5RNi4MfNpiUs4M6bgnCTA1mhnlYR1Z5T1B8G7wcEbB70824pgjj5nSExeB2E0VCyPYKmIEv%2Fp5czn6ZdEDlxmKSRMgTmKebCAWuUx06TMX2rSpU4uz3U%2FSBhbu3wurxts9%2BJkOgi3y0adc6FGJlvHeXVCcLmcDy81YREgpBltxIxctYeES69V8qjr2bNXOIzDhmLypQ4tifReA46NprEJAEuqyBDkhP1OpI8q%2BrWsdPjyBFRLG5YtcKBaK7k9o9ZVgPFOuy55I%2BChWQST6%2BYjhTInZKvxpMRwvRXPneNyM0Qd6EA2Wq7mK3yktQMYZthGOJ4%2FOumccBwfesuvfTaXeKrzSC0nEVU%2B8gxg3CpevVfhiSywr16HXrd4CBCwwlfXWIvrdnFflHiYKC1iLNyyYnrobhLDj%2B1jgaNugaVx%2FcVaQ36pPsTAxaie%2B5aSvQlTpYlxZ4CkBReJ887jmiQ7kMH8FhrfHdJLXgqZ91%2BkfArx%2FfIkwlJF4aoVjiBYptw9LkxnF407boGSNzroSdBEUkiLbrsci8plhHlc5RdofLxFhK%2Bjp9dn2EOC%2FVeT9nN0em5qdbJl4v4yFz3yS75HhLawRQr6SQKETyXoxluhbRuE50Fe7hP4XusiOT2Y6nDBRBJK26ICsfLsw%2BI5hFiCVOjVtuo%2Bg3ZboxUl9zEZNgUjY1rQMV%2F6ZPSDPPff2k27KUNwHaXdhMnQMIfd80pyENBtFu%2FODK1MMDDtrUGOrABm1szP4v6O7NZx3ZO35o%2B8FNW7BZHTMpFeSqCtVZtWX%2Fj2zhoABCxfAQLi13tluIY%2FSkQy0I1WMe89PRrEjzDpSGJa69y3E0yYvc0petzSEi3NJ1fRb1bc9yg45pSGPu3ulj45Rrf1J8QzmaQBOu7gJE7myhhOCvGGLlNaXjh2hQ79A3sWkgZNM4%2Bi6MSZHv%2Fuv7mpqUMGQGSg0ItxmVT9q08MEpxuj4KSlYcbNYy8MA%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240803T040548Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYTT72PNZG%2F20240803%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=20b44e0fbd45f00ac4e55a14c483a8a882f745817cbb97e3bee8254425e0eade&amp;hash=f84d56caa1bef3039f2b686b8bc0e6073bf754f12c789ae4c35066593ae8baf1&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0031320323000699&amp;tid=spdf-84396954-8269-409c-889a-94cc067eb8fd&amp;sid=ced7851952f3524cf51bdf7-ca9e94c8ebbagxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0b175b050656560355&amp;rr=8ad36e702d55aa4a&amp;cc=kr" target=_blank rel=noopener>Liu, Y., Wang, W., Feng, C., Zhang, H., Chen, Z., & Zhan, Y. (2023). Expression snippet transformer for robust video-based facial expression recognition. Pattern Recognition, 138, 109368.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 1, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_04_12_knudsen_wallis_nature_rev_neuro/>[Article] Taking stock of value in the orbitofrontal cortex. Nature Reviews Neuroscience.</a></div><div class=mt-2><p>Summary: The authors reviewed two hypotheses on functions of the OFC, the value hypothesis and the cognitive map hypothesis. They then suggest a new view by reconciling two hypotheses and considering the hippocampal functions.</p><p><a href=https://www.nature.com/articles/s41583-022-00589-2 target=_blank rel=noopener>Knudsen, E. B., & Wallis, J. D. (2022). Taking stock of value in the orbitofrontal cortex. Nature Reviews Neuroscience.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 12, 2024</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/seminar_papers/page/2/>&#171;</a></li><li class=page-item><a class=page-link href=/seminar_papers/page/4/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2025 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e762603fd04b1d81f1e953bafed73e89.js></script></body></html>