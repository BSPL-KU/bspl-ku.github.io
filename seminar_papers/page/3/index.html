<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/seminar_papers/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/seminar_papers/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/seminar_papers/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/seminar_papers/"><meta property="og:title" content="Seminar Papers | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-05-20T00:00:00+00:00"><title>Seminar Papers | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1ef0c7c24519ec8ae8f7b9f6b71a9a5f><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a>
<a class=dropdown-item href=https://github.com/bsplku><span>GitHub</span></a></div></li><li class=nav-item><a class="nav-link active" href=/seminar_papers><span>Seminar Papers</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Seminar Papers</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_05_22_justin-hudak_et_al_neuropsychopharmacology/>[Article] Endogenous theta stimulation during meditation predicts reduced opioid dosing following treatment with Mindfulness-Oriented Recovery Enhancement.</a></div><div class=mt-2><p>Summary: This study investigates the effectiveness of the Mindfulness-Oriented Recovery Enhancement (MORE) program for police opioid users. Experimental results showed that participants exhibited an increase in theta and alpha brain waves during mindfulness meditation, along with improved coherence of mid-frontal theta. These neurophysiological changes were associated with reductions in opioid use, related to self-transcendence induced by mindfulness.</p><p><a href=https://www.nature.com/articles/s41386-020-00831-4 target=_blank rel=noopener>Hudak, J., Hanley, A.W., Marchand, W.R. et al. Endogenous theta stimulation during meditation predicts reduced opioid dosing following treatment with Mindfulness-Oriented Recovery Enhancement. Neuropsychopharmacol. 46, 836–843 (2021)</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 22, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_05_01_liu_et_al_pattern_recognition/>[Article] Expression snippet transformer for robust video-based facial expression recognition</a></div><div class=mt-2><p>Summary: Although Transformer can be powerful for modeling visual relations and describing complicated patterns, it could still perform unsatisfactorily for video-based facial expression recognition, since the expression movements in a video can be too small to reflect meaningful spatial-temporal relations. They propose to decompose the modeling of expression movements of a video into the modeling of a series of expression snippets, each of which contains a few frames. Their propsed model, Expression Snippet Transformer (EST) process intra-snippet and inter-snippet information seperately and combine them together. Code is available in github.</p><p><a href="https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320323X00028/1-s2.0-S0031320323000699/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDt2P6bMhy6Lgb1O9LxiNbyAx4sDh3VBo4a8Ubo4B9ebQIhAP9cmOfvLTtJ7aOxbNWclPNHqqAmPCDjxR1u6ZY5CN%2B3KrsFCJz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igzig%2FFM%2FVlxcsnj5iAqjwW%2Ffco0MPow3tza2ZmvoQu6kgeWn%2F1ipwiNsjqCtFyPqGESVobiKyK2qhL%2BO6gLjuzAZpS5%2BbPsWbRTDZw2TGqFXJ%2BUm3E%2BAbpk6%2FcTLSUJ%2BhcpY212lG9MGzZ455TkmAluQ2crUyVs5RNi4MfNpiUs4M6bgnCTA1mhnlYR1Z5T1B8G7wcEbB70824pgjj5nSExeB2E0VCyPYKmIEv%2Fp5czn6ZdEDlxmKSRMgTmKebCAWuUx06TMX2rSpU4uz3U%2FSBhbu3wurxts9%2BJkOgi3y0adc6FGJlvHeXVCcLmcDy81YREgpBltxIxctYeES69V8qjr2bNXOIzDhmLypQ4tifReA46NprEJAEuqyBDkhP1OpI8q%2BrWsdPjyBFRLG5YtcKBaK7k9o9ZVgPFOuy55I%2BChWQST6%2BYjhTInZKvxpMRwvRXPneNyM0Qd6EA2Wq7mK3yktQMYZthGOJ4%2FOumccBwfesuvfTaXeKrzSC0nEVU%2B8gxg3CpevVfhiSywr16HXrd4CBCwwlfXWIvrdnFflHiYKC1iLNyyYnrobhLDj%2B1jgaNugaVx%2FcVaQ36pPsTAxaie%2B5aSvQlTpYlxZ4CkBReJ887jmiQ7kMH8FhrfHdJLXgqZ91%2BkfArx%2FfIkwlJF4aoVjiBYptw9LkxnF407boGSNzroSdBEUkiLbrsci8plhHlc5RdofLxFhK%2Bjp9dn2EOC%2FVeT9nN0em5qdbJl4v4yFz3yS75HhLawRQr6SQKETyXoxluhbRuE50Fe7hP4XusiOT2Y6nDBRBJK26ICsfLsw%2BI5hFiCVOjVtuo%2Bg3ZboxUl9zEZNgUjY1rQMV%2F6ZPSDPPff2k27KUNwHaXdhMnQMIfd80pyENBtFu%2FODK1MMDDtrUGOrABm1szP4v6O7NZx3ZO35o%2B8FNW7BZHTMpFeSqCtVZtWX%2Fj2zhoABCxfAQLi13tluIY%2FSkQy0I1WMe89PRrEjzDpSGJa69y3E0yYvc0petzSEi3NJ1fRb1bc9yg45pSGPu3ulj45Rrf1J8QzmaQBOu7gJE7myhhOCvGGLlNaXjh2hQ79A3sWkgZNM4%2Bi6MSZHv%2Fuv7mpqUMGQGSg0ItxmVT9q08MEpxuj4KSlYcbNYy8MA%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240803T040548Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYTT72PNZG%2F20240803%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=20b44e0fbd45f00ac4e55a14c483a8a882f745817cbb97e3bee8254425e0eade&amp;hash=f84d56caa1bef3039f2b686b8bc0e6073bf754f12c789ae4c35066593ae8baf1&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0031320323000699&amp;tid=spdf-84396954-8269-409c-889a-94cc067eb8fd&amp;sid=ced7851952f3524cf51bdf7-ca9e94c8ebbagxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0b175b050656560355&amp;rr=8ad36e702d55aa4a&amp;cc=kr" target=_blank rel=noopener>Liu, Y., Wang, W., Feng, C., Zhang, H., Chen, Z., & Zhan, Y. (2023). Expression snippet transformer for robust video-based facial expression recognition. Pattern Recognition, 138, 109368.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 1, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_04_12_knudsen_wallis_nature_rev_neuro/>[Article] Taking stock of value in the orbitofrontal cortex. Nature Reviews Neuroscience.</a></div><div class=mt-2><p>Summary: The authors reviewed two hypotheses on functions of the OFC, the value hypothesis and the cognitive map hypothesis. They then suggest a new view by reconciling two hypotheses and considering the hippocampal functions.</p><p><a href=https://www.nature.com/articles/s41583-022-00589-2 target=_blank rel=noopener>Knudsen, E. B., & Wallis, J. D. (2022). Taking stock of value in the orbitofrontal cortex. Nature Reviews Neuroscience.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 12, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_03_27_conwell_et_al_neurips/>[Article] What can 5.17 billion regression fits tell us about artificial models of the human visual system?</a></div><div class=mt-2><p>Summary: They performed a large-scale benchmarking analysis of 85 modern deep neural network models (e.g. CLIP, BarlowTwins,Mask-RCNN). They found that architectural differences have very little consequence in emergent fits to brain data. Next, differences in task have clear effects–with categorization and self-supervised models showing relatively stronger brain predictivity.</p><p><a href=https://konklab.fas.harvard.edu/Papers/Conwell_2021_SVRHM.pdf target=_blank rel=noopener>Conwell, C., Prince, J. S., Alvarez, G. A., & Konkle, T. (2021, October). What can 5.17 billion regression fits tell us about artificial models of the human visual system?. In SVRHM 2021 Workshop@ NeurIPS.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 27, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_03_13_shi_et_al_cikm_liu_et_al_ecmlpkdd/>[Article] Gigamae: Generalizable graph masked autoencoder via collaborative latent space reconstruction & Masked graph auto-encoder constrained graph pooling.</a></div><div class=mt-2><h3 id=gigamae-generalizable-graph-masked-autoencoder-via-collaborative-latent-space-reconstruction>Gigamae: Generalizable graph masked autoencoder via collaborative latent space reconstruction</h3><p>Summary: GiGaMAE investigated how to enhance the generalization capability of self-supervised graph generative models, by reconstructing graph information in the latent space. They proposed a nove self-supervised reconstruction loss.</p><p><a href=https://dl.acm.org/doi/abs/10.1145/3583780.3614894 target=_blank rel=noopener>Shi, Yucheng, et al. &ldquo;Gigamae: Generalizable graph masked autoencoder via collaborative latent space reconstruction.&rdquo; Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 2023.</a></p><h3 id=masked-graph-auto-encoder-constrained-graph-pooling>Masked graph auto-encoder constrained graph pooling."</h3><p>Summary: MGAP is the novel node drop pooling method retaining sufficient effective graph information from both node attribute and network topology perspectives.</p><p><a href=https://link.springer.com/chapter/10.1007/978-3-031-26390-3_23 target=_blank rel=noopener>Liu, Chuang, et al. &ldquo;Masked graph auto-encoder constrained graph pooling.&rdquo; Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer International Publishing, 2022.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 13, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_01_03_tomov_et_al_neuron/>[Article] The neural architecture of theory-based reinforcement learning. Neuron.</a></div><div class=mt-2><p>Summary: The authors compared the theory-based RL model called Explore, Model, Plan Agent (EMPA) on Atari games with human performance and the double DQN model. The EMPA model showed compatible performances with human participants. Encoding analyses identified neural representation associated with theory-encoding and theory-updating.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S0896627323000739 target=_blank rel=noopener>Tomov, M. S., Tsividis, P. A., Pouncy, T., Tenenbaum, J. B., & Gershman, S. J. (2023). The neural architecture of theory-based reinforcement learning. Neuron.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jan 3, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2023_11_22_wen_et_al_ieee/>[Article] Graph Self-supervised Learning with Application to Brain Networks Analysis. IEEE Journal of Biomedical and Health Informatics.</a></div><div class=mt-2><p>Summary: They suggested BrainGSLs to capture more information in limited data and insufficient supervision. It incorporates a local topological-aware encoder, a node-edge bi-decoder, a signal representation learning module, and a classifier. They evaluated their model on ASD, BD, and MDD datasets.</p><p><a href=https://ieeexplore.ieee.org/abstract/document/10122156 target=_blank rel=noopener>Wen, G., Cao, P., Liu, L., Yang, J., Zhang, X., Wang, F., & Zaiane, O. R. (2023). Graph Self-supervised Learning with Application to Brain Networks Analysis. IEEE Journal of Biomedical and Health Informatics.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Nov 22, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2023_10_04_li_jingting_et_al/>[Article] CAS(ME)3: A third generation facial spontaneous micro-expression database with depth information and high ecological validity.</a></div><div class=mt-2><p>summary : Recently, Micro-Expression(ME) is one of the popular research interest for Facial Expression Recognition(FER) task. In particular, depth information is often utilized to analyze micro expressions. CAS(ME)3 offers around 80 hours of video dataset with manually labelled micro-expressions & macro-expressions. They also provide depth information and demonstrate effective way to process depth information for multimodal Mircro Expression Recognition(MER). CAS(ME)3 is currently one of the most well-known RGB-D dataset for emotion recognition.</p><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774929" target=_blank rel=noopener>Li, J., Dong, Z., Lu, S., Wang, S. J., Yan, W. J., Ma, Y., &mldr; & Fu, X. (2022). CAS (ME) 3: A third generation facial spontaneous micro-expression database with depth information and high ecological validity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 2782-2800.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Oct 4, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2023_08_16_khait_et_al_cell/>[Article] Sounds emitted by plants under stress are airborne and informative.</a></div><div class=mt-2><p>summary : Plants emit remotely detectable and informative airborne sounds under stress. Plants are not quite, human just cannot listen! With this experiments, these sound could be detected from a distance of 3–5m by many mammals and insects, which can make them interact with plant</p><p><a href=https://www.cell.com/cell/pdf/S0092-8674%2823%2900262-3.pdf target=_blank rel=noopener>Khait, I., Lewin-Epstein, O., Sharon, R., Saban, K., Goldstein, R., Anikster, Y., &mldr; & Hadany, L. (2023). Sounds emitted by plants under stress are airborne and informative. Cell, 186(7), 1328-1336.</a>)</p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Aug 16, 2023</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2023_08_09_goh_et_al_proceedings_of_the_national_academy_of_sciences/>[Article] The perception of silence.</a></div><div class=mt-2><p>summary : This study investigates the perception of silence by employing the auditory illusion paradigm. The hypothesis posits that if silence can be perceived, then the same auditory illusion would exist. Therefore, silence was used instead of auditory stimuli to induce auditory illusions. Participants reported experiencing the same illusions as in the normal auditory illusion paradigm</p><p><a href=https://www.pnas.org/doi/epdf/10.1073/pnas.2301463120 target=_blank rel=noopener>Goh, R. Z., Phillips, I. B., & Firestone, C. (2023). The perception of silence. Proceedings of the National Academy of Sciences, 120(29), e2301463120.</a>)</p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Aug 9, 2023</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/seminar_papers/page/2/>&#171;</a></li><li class=page-item><a class=page-link href=/seminar_papers/page/4/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2025 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e762603fd04b1d81f1e953bafed73e89.js></script></body></html>