<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/seminar_papers/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/seminar_papers/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/seminar_papers/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/seminar_papers/"><meta property="og:title" content="Seminar Papers | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-03-25T00:00:00+00:00"><title>Seminar Papers | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1ef0c7c24519ec8ae8f7b9f6b71a9a5f><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a>
<a class=dropdown-item href=https://github.com/bsplku><span>GitHub</span></a></div></li><li class=nav-item><a class="nav-link active" href=/seminar_papers><span>Seminar Papers</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Seminar Papers</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_09_21_parkes_et_al_translationalpsychiatry/>[Article] Transdiagnostic dimensions of psychopathology explain individuals’ unique deviations from normative neurodevelopment in brain structure.</a></div><div class=mt-2><p>Summary: Heterogeneity and case-control approaches to mental disorders have made it hard to link dimensions of psychopathology to abnormalities of neurodevelopment. In this study, they tried to find psychiatric biomarkers with normative modeling and machine learning using cortical volume. They showed that modeling cortical volume as deviations from normative models of neurodevelopment improved the prediction of overall psychopathology (p-factor). Also, they showed detailed group differences between MDD/ADHD and healthy groups, suggesting that the p-factor confounded case-control comparisons.</p><p><a href=https://www.nature.com/articles/s41398-021-01342-6 target=_blank rel=noopener>Parkes, L., Moore, T. M., Calkins, M. E., Cook, P. A., Cieslak, M., Roalf, D. R., &mldr; & Bassett, D. S. (2021). Transdiagnostic dimensions of psychopathology explain individuals’ unique deviations from normative neurodevelopment in brain structure. Translational psychiatry, 11(1), 1-13.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Sep 21, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_09_14_li_et_al_medicalimageanalysis/>[Article] BrainGNN: Interpretable Brain Graph Neural Network for fMRI Analysis.</a></div><div class=mt-2><p>summary : They proposed BrainGNN, which is a graph neural network framework to analyze fMRI and discover neurological biomarkers. BrainGNN contains novel ROI-aware graph convolutional (Ra-GConv) layers that leverage the topological and functional information of fMRI. They applied the BrainGNN framework on two independent fMRI datasets(ASD fMRI dataset and HCP 900 subject release).</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1361841521002784#! target=_blank rel=noopener>Li, Xiaoxiao, et al. &ldquo;Braingnn: Interpretable brain graph neural network for fmri analysis.&rdquo; Medical Image Analysis 74 (2021): 102233.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Sep 14, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_09_07_ngai_wang_kay_et_al_nature_communication/>[Article] Emotion recognition based on convolutional neural networks and heterogeneous bio-signal data sources.</a></div><div class=mt-2><p>summary : This paper uses multi-modal bio-signals such as EEG, Eye data (Pupil diameter, eye gaze coordinates), and Facial data (using a depth camera) to recognize an individual&rsquo;s emotions in valence and arousal. This paper suggests the multi-branch convolutional neural network (MBCNN) which shows the best accuracy among the state-of-the-art models while using multi-modal data, especially depth data.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1566253521001457 target=_blank rel=noopener>Ngai, Wang Kay, et al. &ldquo;Emotion recognition based on convolutional neural networks and heterogeneous bio-signal data sources.&rdquo; Information Fusion 77 (2022): 107-117.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Sep 7, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_07_06_li_et_al_nature_communication/>[Article] Cortical structural differences in major depressive disorder correlate with cell type-specific transcriptional signatures.</a></div><div class=mt-2><p>summary : They examine the link between brain-wide gene expression and morphometric changes in individuals with MDD. They observe that the expression of MDD-associated genes spatially correlates with MSN differences.</p><p><a href=https://www.nature.com/articles/s41467-021-21943-5 target=_blank rel=noopener>Li, Jiao, et al. &ldquo;Cortical structural differences in major depressive disorder correlate with cell type-specific transcriptional signatures.&rdquo; Nature communications 12.1 (2021): 1-14.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jul 6, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_06_08_lee_et_al_biological_psychiatry/>[Article] Genetic Association of Attention-Deficit/Hyperactivity Disorder and Major Depression With Suicidal Ideation and Attempts in Children: The Adolescent Brain Cognitive Development Study.</a></div><div class=mt-2><p>Summary: Suicidal attempt or ideation is well known associated with various environmental factors and psychopathology. In this study, they examined whether genetic susceptibility to major psychiatric disorders is associated with suicidal behaviors. Using polygenic risk scores and suicide risk measures from ABCD KSADS data, they found that MDD and ADHD are heavily associated with suicidal risks, internalizing and externalizing respectively.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S0006322321018643?via%3Dihub target=_blank rel=noopener>Lee, P. H., Doyle, A. E., Li, X., Silberstein, M., Jung, J. Y., Gollub, R. L., &mldr; & Fava, M. (2021). Genetic Association of Attention-Deficit/Hyperactivity Disorder and Major Depression With Suicidal Ideation and Attempts in Children: The Adolescent Brain Cognitive Development Study. Biological Psychiatry.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jun 8, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_05_25_stein_et_al_mindfulness/>[Article] Tentative fMRI signatures of perceptual echoes in early visual cortex.</a></div><div class=mt-2><p>Summary: The aim of study is to examine the association between dispositional mindfulness and PFC neural activity during working memory and identify the dispositional mindfulness from AAMS (Adult and Adolescent Mindfulness Scale) that would be associated with greater working memory performance. The result showed the decreased BOLD signal in the right vlPFC related to higher Attention and Awareness score and reduced FC between right vlPFC and dmPFC related to higher Nonreactivity.</p><p><a href=https://link.springer.com/article/10.1007/s12671-021-01785-4 target=_blank rel=noopener>Stein, J. A., Bray, S., MacMaster, F. P., Tomfohr-Madsen, L., & Kopala-Sibley, D. C. (2022). Adolescents with High Dispositional Mindfulness Show Altered Right Ventrolateral Prefrontal Cortex Activity During a Working Memory Task. Mindfulness, 13(1), 198-210.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 25, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_05_18_oblak_et_al_neuroimage_2019/>[Article] A simulation-based approach to improve decoded neurofeedback performance.</a></div><div class=mt-2><p>Summary: in real-time neurofeedback experiments, there is not yet to be an empirical justification of the timing and data processing parameters. theses parameters and timing is important. so, they investigate how design parameters of decoded neurofeedback experiments affect accuracy and neurofeedback performance. and they demonstrate the usefulness of offline simulation to improve the success of real-time neurofeedback experiments.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1053811919302629 target=_blank rel=noopener>Oblak, Ethan F., James S. Sulzer, and Jarrod A. Lewis-Peacock. &ldquo;A simulation-based approach to improve decoded neurofeedback performance.&rdquo; NeuroImage 195 (2019): 300-310.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 18, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_05_11_luo_canhuang_et_al_neuroimage/>[Article] Tentative fMRI signatures of perceptual echoes in early visual cortex.</a></div><div class=mt-2><p>Summary: Here, they conducted an EEG and fMRI experiment to investigate the neural basis of the impulse response function(IRF). They measured the IRF of each subject in the EEG session and then reconstructed an estimate of the EEG signal by convolving the IRF with the stimuli presented in the fMRI session. The envelope of reconstructed EEG signals in the theta, alpha, and beta bands was taken as regressors for the GLM. They found the envelope of the EEG alpha positively correlated with BOLD activity in V1 and V2, but not with activity in the retinotopically stimulated regions.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S105381192100330X#bib0014 target=_blank rel=noopener>Luo, Canhuang, et al. &ldquo;Tentative fMRI signatures of perceptual echoes in early visual cortex.&rdquo; NeuroImage 237 (2021): 118053.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 11, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_05_04_jacob_et_al_neuroimage/>[Article] Aperiodic measures of neural excitability are associated with anticorrelated hemodynamic networks at rest: a combined EEG-fMRI study</a></div><div class=mt-2><p>Summary: They investigated periodic and aperiodic EEG parameters associated with distinct resting state networks and used simultaneous EEG-fMRI recording (resting state). They found that increases in aperiodic power is associated with an auditory-salience-cerebellar network and decreases in aperiodic power is associated with prefrontal regions. Also, they found that global neural excitability may reflect stimulus processing or arousal attributable to the uniqueness of the resting-state MR-scanner environment.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1053811921009770#cebibl1 target=_blank rel=noopener>Jacob, Michael S., et al. &ldquo;Aperiodic measures of neural excitability are associated with anticorrelated hemodynamic networks at rest: a combined EEG-fMRI study.&rdquo; NeuroImage 245 (2021): 118705.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 4, 2022</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2022_04_27_siddharth_et_al_ieee_affectivecomputing/>[Article] Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing</a></div><div class=mt-2><p>Summary: In this study, they applied novel feature extraction and deep-learning methods to 4 public datasets including DEAP and MAHNOB-HCI for multimodal emotion classification. They proposed utilization of pre-trained VGG-net to compensate for data shortage in bio-sensing field. A wide range of modalities was used including EEG, HRV, GSR and face videos. They evaluate accuracy of single modality, combination of datasets in feature level and transfer learning. Result outperformed previous studies.</p><p><a href=https://ieeexplore.ieee.org/abstract/document/8713896 target=_blank rel=noopener>Siddharth, S., Jung, T. P., & Sejnowski, T. J. (2019). Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing. IEEE Transactions on Affective Computing.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 27, 2022</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/seminar_papers/page/4/>&#171;</a></li><li class=page-item><a class=page-link href=/seminar_papers/page/6/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2025 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e762603fd04b1d81f1e953bafed73e89.js></script></body></html>