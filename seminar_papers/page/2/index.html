<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/seminar_papers/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/seminar_papers/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/seminar_papers/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/seminar_papers/"><meta property="og:title" content="Seminar Papers | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-10-21T00:00:00+00:00"><title>Seminar Papers | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1ef0c7c24519ec8ae8f7b9f6b71a9a5f><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a>
<a class=dropdown-item href=https://github.com/bsplku><span>GitHub</span></a></div></li><li class=nav-item><a class="nav-link active" href=/seminar_papers><span>Seminar Papers</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Seminar Papers</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_05_20_qiu_et_al_arxiv/>[Article] MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding</a></div><div class=mt-2><p>Summary: In this seminar, we will explore a novel fMRI-to-text decoding framework named MindLLM. It combines a neuroscience-informed, subject-agnostic fMRI encoder with an off-the-shelf large language model to translate brain activity into coherent text. It introduces Brain Instruction Tuning (BIT), which enriches the modelâ€™s capacity to extract and represent diverse semantic information from fMRI signals, enabling versatile decoding across different tasks and subjects. We will discuss how to implement these techniques in our study.
<a href=https://arxiv.org/abs/2502.15786 target=_blank rel=noopener>Qiu, Weikang, et al. &ldquo;MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding.&rdquo; arXiv preprint arXiv:2502.15786 (2025).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>May 20, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_04_29_koush_et_al_neuroimage/>[Article] OpenNFT: An open-source Python/Matlab framework for real-time fMRI neurofeedback training based on activity, connectivity and multivariate pattern analysis</a></div><div class=mt-2><p>Summary: In this seminar, we will explore the structure of an open-source rtfMRI-NF toolbox. They use Python and MATLAB to preprocess data, generate feedback, and provide it.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1053811917305050 target=_blank rel=noopener>Koush, Yury, et al. &ldquo;OpenNFT: An open-source Python/Matlab framework for real-time fMRI neurofeedback training based on activity, connectivity and multivariate pattern analysis.&rdquo; NeuroImage 156 (2017): 489-503.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 29, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_04_15_wang_et_al_arxiv/>[Article] Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models</a></div><div class=mt-2><p>Summary: In this study, to enhance our understanding of visual processes, they developed WAVE, which reconstructs visual stimuli from fMRI data. By integrating three modalities (fMRI, image, and text) to perform contrastive learning, the features are then passed to a diffusion model for final image reconstruction.</p><p><a href=https://arxiv.org/abs/2411.07121 target=_blank rel=noopener>Wang, Turnbull, et al. &ldquo;Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models.&rdquo; arXiv(2024)</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 15, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_04_15_wang_yanchen_et_al_arxiv/>[Article] Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models</a></div><div class=mt-2><p>Summary: In this study, to enhance our understanding of visual processes, they developed WAVE, which reconstructs visual stimuli from fMRI data. By integrating three modalities (fMRI, image, and text) to perform contrastive learning, the features are then passed to a diffusion model for final image reconstruction.
<a href=https://arxiv.org/abs/2411.07121 target=_blank rel=noopener>Wang, Y., Turnbull, A., Xiang, T., Xu, Y., Zhou, S., Masoud, A., &mldr; & Adeli, E. (2024). Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models. arXiv preprint arXiv:2411.07121.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 15, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_03_25_liu_et_al_cell/>[Article] Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations</a></div><div class=mt-2><p>Summary: The study performed traditional binary GWAS, continuous univariate GWAS using wearable combination scores, and multivariate GWAS to identify genetic variants associated with ADHD. The identified variants showed associations with heart function (MYH6, CMTM5) and ADHD-related genes (ELFN1), with some variants potentially having a protective effect against ADHD.</p><p><a href=https://www.cell.com/cell/fulltext/S0092-8674%2824%2901329-1 target=_blank rel=noopener>Liu, Jason J., et al. &ldquo;Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations.&rdquo; Cell (2024).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 25, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_03_12_thapaliya_et_al_medical_image_analysis/>[Article] Brain networks and intelligence: A graph neural network based approach to resting state fmri data</a></div><div class=mt-2><p>Summary: This study introduces BrainRGIN, a novel graph neural network (GNN) model designed to predict intelligence using resting-state fMRI data. By leveraging graph isomorphism networks and clustering-based embeddings, the model effectively captures brain sub-network structures. The authors validate their approach using the Adolescent Brain Cognitive Development Dataset and demonstrate superior predictive performance compared to traditional machine learning models.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S136184152400358X target=_blank rel=noopener>Thapaliya, Bishal, et al. &ldquo;Brain networks and intelligence: A graph neural network based approach to resting state fmri data.&rdquo; Medical Image Analysis 101 (2025): 103433.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 12, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_02_25_shao_et_al_minfulness_balters_et_al_acm_interactive_rahman_et_al_acm_hci/>[Article] The Effect of Slow-Paced Breathing on Cardiovascular and Emotion Functions: A Meta-Analysis and Systematic Review & Calm Commute: Guided Slow Breathing for Daily Stress Management in Drivers & BreatheBuddy: Tracking Real-time Breathing Exercises for Automated Biofeedback Using Commodity Earbuds.</a></div><div class=mt-2><h3 id=the-effect-of-slow-paced-breathing-on-cardiovascular-and-emotion-functions-a-meta-analysis-and-systematic-review>The Effect of Slow-Paced Breathing on Cardiovascular and Emotion Functions: A Meta-Analysis and Systematic Review</h3><p>Summary: A meta-analysis of existing literature on the effects of slow-paced breathing on cardiovascular indices, including HR, HRV, and BP, as well as on negative emotions.The training showed a moderate effect in reducing SBP, moderate-to-large effect in increasing time-domain HRV, and a small effect in reducing HR.Also, slow-paced breathing may reduce negative emotions such as perceived stress.Long-term effect of slow-paced breathing was found reducing SBP and DBP among prehypertensive subjects.
<a href=https://link.springer.com/article/10.1007/s12671-023-02294-2 target=_blank rel=noopener>Shao, R., Man, I.S.C. & Lee, T.M.C. The Effect of Slow-Paced Breathing on Cardiovascular and Emotion Functions: A Meta-Analysis and Systematic Review. Mindfulness 15, 1â€“18 (2024).</a></p><h3 id=calm-commute-guided-slow-breathing-for-daily-stress-management-in-drivers>Calm Commute: Guided Slow Breathing for Daily Stress Management in Drivers</h3><p>Summary: This study presented the first controlled study of a short, on-road breathing intervention with both calm and stressful driving conditions with a sample of experienced drivers familiar with the regular, daily, commuting experience in the US.Their stress inducing task confirmed that people were more stressed during the stressor inducing condition.Also, for those who engaged with intervention showed decrease in breathing rate during normal driving was about 15% or about one half of the intended decrease.</p><p><a href=https://dl.acm.org/doi/abs/10.1145/3380998 target=_blank rel=noopener>Balters, Stephanie, et al. &ldquo;Calm commute: Guided slow breathing for daily stress management in drivers.&rdquo; Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4.1 (2020): 1-19.</a></p><h3 id=breathebuddy-tracking-real-time-breathing-exercises-for-automated-biofeedback-using-commodity-earbuds>BreatheBuddy: Tracking Real-time Breathing Exercises for Automated Biofeedback Using Commodity Earbuds</h3><p>Summary: Here they presented BreatheBuddy, a passive respiratory sensing system that monitors comprehensive breathing biomarkers during breathing exercises in real-time using earbudâ€™s accelerometer.Their evaluation with independent test users shows quite accurate performances. The interfaces they presented facilitates real-time breathing biofeedback to potentially make earbud as an effective tool for breathing exercises towards stress relaxation.</p><p><a href=https://dl.acm.org/doi/abs/10.1145/3546748 target=_blank rel=noopener>Rahman, Md Mahbubur, et al. &ldquo;Breathebuddy: Tracking real-time breathing exercises for automated biofeedback using commodity earbuds.&rdquo; Proceedings of the ACM on human-computer interaction 6.MHCI (2022): 1-18.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Feb 25, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_02_04_steffen_et_al_frontiers_psychology_schumann_et_al_scientific_data/>[Article] Integrating breathing techniques into psychoptherapy to improve HRV: which approach is best? & One-week test-retest recordings of resting cardiorespiratory data for reliability analysis</a></div><div class=mt-2><h3 id=integrating-breathing-techniques-into-psychoptherapy-to-improve-hrv-which-approach-is-best>Integrating Breathing Techniques Into Psychoptherapy to Improve HRV: Which Approach Is Best?</h3><p>Summary: This study examined the effects of six breaths per minute breathing, soothing rhythm breathing, and nature video viewing on HRV, respiratory rate, and emotional regulation through a randomized controlled experiment. The results showed that both breathing techniques significantly increased HRV (SDNN, LF HRV, LF/HF ratio), with six breaths per minute breathing being the most effective, while nature video viewing had no impact; however, HF HRV did not show significant changes across conditions, and emotional responses to breathing exercises were not significantly different between clinically at-risk and non-at-risk participants.</p><p><a href=https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.624254/full target=_blank rel=noopener>Steffen, Patrick R., et al. &ldquo;Integrating breathing techniques into psychotherapy to improve HRV: which approach is best?.&rdquo; Frontiers in Psychology 12 (2021): 624254.</a></p><h3 id=one-week-test-retest-recordings-of-resting-cardiorespiratory-data-for-reliability-analysis>One-week test-retest recordings of resting cardiorespiratory data for reliability analysis</h3><p>Summary: This study evaluates the test-retest reliability of heart rate variability (HRV) and cardiopulmonary coupling in healthy individuals by analyzing ECG and respiration data collected one week apart. The results indicate high reliability for mean heart rate (HR), but greater variability in RMSSD, highlighting the importance of careful HRV metric selection in research.</p><p><a href=https://www.nature.com/articles/s41597-024-04303-y target=_blank rel=noopener>Schumann, Andy, et al. &ldquo;One-week test-retest recordings of resting cardiorespiratory data for reliability analysis.&rdquo; Scientific Data 12.1 (2025): 12.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Feb 4, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_01_14_rafailov_et_al_advances_in_neural_information_processing_systems_gulcehre_et_al_arxiv_yang_et_al_arxiv/>[Article] Direct preference optimization: Your language model is secretly a reward model & Reinforced self-training (rest) for language modeling & Rlcd: Reinforcement learning from contrast distillation for language model alignment</a></div><div class=mt-2><h3 id=direct-preference-optimization-your-language-model-is-secretly-a-reward-model>Direct preference optimization: Your language model is secretly a reward model</h3><p>Summary: This paper proposes a method to directly align language models using human preference data without reinforcement learning. It simplifies the optimization process by utilizing a binary classification loss based on human preferences, eliminating the need for explicit reward model training. This approach is simpler, more stable, and more efficient than RLHF.</p><p><a href=https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html target=_blank rel=noopener>Rafailov, Rafael, et al. &ldquo;Direct preference optimization: Your language model is secretly a reward model.&rdquo; Advances in Neural Information Processing Systems 36 (2024).</a></p><h3 id=reinforced-self-training-rest-for-language-modeling>Reinforced self-training (rest) for language modeling</h3><p>Summary: ReST is a reinforcement learning algorithm that improves language model policies by sampling outputs from an initial model and refining them using offline RL. It enhances data reusability, reduces computational costs, and aligns outputs with human preferences.</p><p><a href=https://arxiv.org/abs/2308.08998 target=_blank rel=noopener>Gulcehre, Caglar, et al. &ldquo;Reinforced self-training (rest) for language modeling.&rdquo; arXiv preprint arXiv:2308.08998 (2023).</a></p><h3 id=rlcd-reinforcement-learning-from-contrast-distillation-for-language-model-alignment>Rlcd: Reinforcement learning from contrast distillation for language model alignment</h3><p>Summary: RLCD aligns language models using contrastive distillation by generating preference data from positive and negative prompts. A reward model trained on these preferences refines the model via reinforcement learning. This approach reduces noise in preference data and achieves superior alignment compared to RLHF.</p><p><a href=https://arxiv.org/abs/2307.12950 target=_blank rel=noopener>Yang, Kevin, et al. &ldquo;Rlcd: Reinforcement learning from contrast distillation for language model alignment.&rdquo; arXiv preprint arXiv:2307.12950 (2023).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jan 14, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_12_13_turini_et_al_scientific_reports/>[Article] Hierarchical organization of objects in scenes is reflected in mental representations of objects</a></div><div class=mt-2><p>summary: This study explored how the hierarchical structure of scene grammar is reflected in our object recognition. Scenes are divided into several &ldquo;phrases,&rdquo; each consisting of a central &ldquo;anchor&rdquo; object and its surrounding &ldquo;local objects.&rdquo; Participants consistently judged object pairs within the same phrase to be more similar, and this tendency was observed across both images and words. This demonstrates that the hierarchical structure of the visual environment is integrated into our abstract mental representations. Consequently, this study is expected to provide insights into how stimuli can be extracted from natural scene data, such as COCO images.</p><p><a href=https://www.nature.com/articles/s41598-022-24505-x target=_blank rel=noopener>Turini, Jacopo, and Melissa Le-Hoa VÃµ. &ldquo;Hierarchical organization of objects in scenes is reflected in mental representations of objects.&rdquo; Scientific Reports 12.1 (2022): 20068.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Dec 13, 2024</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/seminar_papers/>&#171;</a></li><li class=page-item><a class=page-link href=/seminar_papers/page/3/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Â© 2025 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e762603fd04b1d81f1e953bafed73e89.js></script></body></html>