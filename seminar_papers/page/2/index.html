<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=description content><link rel=alternate hreflang=en-us href=https://bspl.korea.ac.kr/seminar_papers/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d4217de0a97b753d87af942d59c81115.css><link rel=alternate href=/seminar_papers/index.xml type=application/rss+xml title="Brain Signal Processing Lab"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://bspl.korea.ac.kr/seminar_papers/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Brain Signal Processing Lab"><meta property="og:url" content="https://bspl.korea.ac.kr/seminar_papers/"><meta property="og:title" content="Seminar Papers | Brain Signal Processing Lab"><meta property="og:description" content><meta property="og:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://bspl.korea.ac.kr/media/icon_hu746e86b219933ead05b970d8e60fbf48_161589_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-08-26T00:00:00+00:00"><title>Seminar Papers | Brain Signal Processing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1ef0c7c24519ec8ae8f7b9f6b71a9a5f><script src=/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Brain Signal Processing Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/aims><span>BSPL is..</span></a></li><li class=nav-item><a class=nav-link href=/members><span>Members</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publications/articles><span>Articles</span></a>
<a class=dropdown-item href=/publications/domestic_conferences><span>Domestic Conferences</span></a>
<a class=dropdown-item href=/publications/international_conferences><span>International Conferences</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Softwares</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/softwares/dnn><span>DNN</span></a>
<a class=dropdown-item href=/softwares/idr><span>iDR</span></a>
<a class=dropdown-item href=/softwares/rspca><span>rsPCA</span></a>
<a class=dropdown-item href=/softwares/env><span>ENV</span></a>
<a class=dropdown-item href=https://github.com/bsplku><span>GitHub</span></a></div></li><li class=nav-item><a class="nav-link active" href=/seminar_papers><span>Seminar Papers</span></a></li><li class=nav-item><a class=nav-link href=/news><span>Lab News</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Seminar Papers</h1></div><div class=universal-wrapper><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_03_12_thapaliya_et_al_medical_image_analysis/>[Article] Brain networks and intelligence: A graph neural network based approach to resting state fmri data</a></div><div class=mt-2><p>Summary: This study introduces BrainRGIN, a novel graph neural network (GNN) model designed to predict intelligence using resting-state fMRI data. By leveraging graph isomorphism networks and clustering-based embeddings, the model effectively captures brain sub-network structures. The authors validate their approach using the Adolescent Brain Cognitive Development Dataset and demonstrate superior predictive performance compared to traditional machine learning models.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S136184152400358X target=_blank rel=noopener>Thapaliya, Bishal, et al. &ldquo;Brain networks and intelligence: A graph neural network based approach to resting state fmri data.&rdquo; Medical Image Analysis 101 (2025): 103433.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Mar 12, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_02_25_shao_et_al_minfulness_balters_et_al_acm_interactive_rahman_et_al_acm_hci/>[Article] The Effect of Slow-Paced Breathing on Cardiovascular and Emotion Functions: A Meta-Analysis and Systematic Review & Calm Commute: Guided Slow Breathing for Daily Stress Management in Drivers & BreatheBuddy: Tracking Real-time Breathing Exercises for Automated Biofeedback Using Commodity Earbuds.</a></div><div class=mt-2><h3 id=the-effect-of-slow-paced-breathing-on-cardiovascular-and-emotion-functions-a-meta-analysis-and-systematic-review>The Effect of Slow-Paced Breathing on Cardiovascular and Emotion Functions: A Meta-Analysis and Systematic Review</h3><p>Summary: A meta-analysis of existing literature on the effects of slow-paced breathing on cardiovascular indices, including HR, HRV, and BP, as well as on negative emotions.The training showed a moderate effect in reducing SBP, moderate-to-large effect in increasing time-domain HRV, and a small effect in reducing HR.Also, slow-paced breathing may reduce negative emotions such as perceived stress.Long-term effect of slow-paced breathing was found reducing SBP and DBP among prehypertensive subjects.
<a href=https://link.springer.com/article/10.1007/s12671-023-02294-2 target=_blank rel=noopener>Shao, R., Man, I.S.C. & Lee, T.M.C. The Effect of Slow-Paced Breathing on Cardiovascular and Emotion Functions: A Meta-Analysis and Systematic Review. Mindfulness 15, 1–18 (2024).</a></p><h3 id=calm-commute-guided-slow-breathing-for-daily-stress-management-in-drivers>Calm Commute: Guided Slow Breathing for Daily Stress Management in Drivers</h3><p>Summary: This study presented the first controlled study of a short, on-road breathing intervention with both calm and stressful driving conditions with a sample of experienced drivers familiar with the regular, daily, commuting experience in the US.Their stress inducing task confirmed that people were more stressed during the stressor inducing condition.Also, for those who engaged with intervention showed decrease in breathing rate during normal driving was about 15% or about one half of the intended decrease.</p><p><a href=https://dl.acm.org/doi/abs/10.1145/3380998 target=_blank rel=noopener>Balters, Stephanie, et al. &ldquo;Calm commute: Guided slow breathing for daily stress management in drivers.&rdquo; Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4.1 (2020): 1-19.</a></p><h3 id=breathebuddy-tracking-real-time-breathing-exercises-for-automated-biofeedback-using-commodity-earbuds>BreatheBuddy: Tracking Real-time Breathing Exercises for Automated Biofeedback Using Commodity Earbuds</h3><p>Summary: Here they presented BreatheBuddy, a passive respiratory sensing system that monitors comprehensive breathing biomarkers during breathing exercises in real-time using earbud’s accelerometer.Their evaluation with independent test users shows quite accurate performances. The interfaces they presented facilitates real-time breathing biofeedback to potentially make earbud as an effective tool for breathing exercises towards stress relaxation.</p><p><a href=https://dl.acm.org/doi/abs/10.1145/3546748 target=_blank rel=noopener>Rahman, Md Mahbubur, et al. &ldquo;Breathebuddy: Tracking real-time breathing exercises for automated biofeedback using commodity earbuds.&rdquo; Proceedings of the ACM on human-computer interaction 6.MHCI (2022): 1-18.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Feb 25, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_02_04_steffen_et_al_frontiers_psychology_schumann_et_al_scientific_data/>[Article] Integrating breathing techniques into psychoptherapy to improve HRV: which approach is best? & One-week test-retest recordings of resting cardiorespiratory data for reliability analysis</a></div><div class=mt-2><h3 id=integrating-breathing-techniques-into-psychoptherapy-to-improve-hrv-which-approach-is-best>Integrating Breathing Techniques Into Psychoptherapy to Improve HRV: Which Approach Is Best?</h3><p>Summary: This study examined the effects of six breaths per minute breathing, soothing rhythm breathing, and nature video viewing on HRV, respiratory rate, and emotional regulation through a randomized controlled experiment. The results showed that both breathing techniques significantly increased HRV (SDNN, LF HRV, LF/HF ratio), with six breaths per minute breathing being the most effective, while nature video viewing had no impact; however, HF HRV did not show significant changes across conditions, and emotional responses to breathing exercises were not significantly different between clinically at-risk and non-at-risk participants.</p><p><a href=https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.624254/full target=_blank rel=noopener>Steffen, Patrick R., et al. &ldquo;Integrating breathing techniques into psychotherapy to improve HRV: which approach is best?.&rdquo; Frontiers in Psychology 12 (2021): 624254.</a></p><h3 id=one-week-test-retest-recordings-of-resting-cardiorespiratory-data-for-reliability-analysis>One-week test-retest recordings of resting cardiorespiratory data for reliability analysis</h3><p>Summary: This study evaluates the test-retest reliability of heart rate variability (HRV) and cardiopulmonary coupling in healthy individuals by analyzing ECG and respiration data collected one week apart. The results indicate high reliability for mean heart rate (HR), but greater variability in RMSSD, highlighting the importance of careful HRV metric selection in research.</p><p><a href=https://www.nature.com/articles/s41597-024-04303-y target=_blank rel=noopener>Schumann, Andy, et al. &ldquo;One-week test-retest recordings of resting cardiorespiratory data for reliability analysis.&rdquo; Scientific Data 12.1 (2025): 12.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Feb 4, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2025_01_14_rafailov_et_al_advances_in_neural_information_processing_systems_gulcehre_et_al_arxiv_yang_et_al_arxiv/>[Article] Direct preference optimization: Your language model is secretly a reward model & Reinforced self-training (rest) for language modeling & Rlcd: Reinforcement learning from contrast distillation for language model alignment</a></div><div class=mt-2><h3 id=direct-preference-optimization-your-language-model-is-secretly-a-reward-model>Direct preference optimization: Your language model is secretly a reward model</h3><p>Summary: This paper proposes a method to directly align language models using human preference data without reinforcement learning. It simplifies the optimization process by utilizing a binary classification loss based on human preferences, eliminating the need for explicit reward model training. This approach is simpler, more stable, and more efficient than RLHF.</p><p><a href=https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html target=_blank rel=noopener>Rafailov, Rafael, et al. &ldquo;Direct preference optimization: Your language model is secretly a reward model.&rdquo; Advances in Neural Information Processing Systems 36 (2024).</a></p><h3 id=reinforced-self-training-rest-for-language-modeling>Reinforced self-training (rest) for language modeling</h3><p>Summary: ReST is a reinforcement learning algorithm that improves language model policies by sampling outputs from an initial model and refining them using offline RL. It enhances data reusability, reduces computational costs, and aligns outputs with human preferences.</p><p><a href=https://arxiv.org/abs/2308.08998 target=_blank rel=noopener>Gulcehre, Caglar, et al. &ldquo;Reinforced self-training (rest) for language modeling.&rdquo; arXiv preprint arXiv:2308.08998 (2023).</a></p><h3 id=rlcd-reinforcement-learning-from-contrast-distillation-for-language-model-alignment>Rlcd: Reinforcement learning from contrast distillation for language model alignment</h3><p>Summary: RLCD aligns language models using contrastive distillation by generating preference data from positive and negative prompts. A reward model trained on these preferences refines the model via reinforcement learning. This approach reduces noise in preference data and achieves superior alignment compared to RLHF.</p><p><a href=https://arxiv.org/abs/2307.12950 target=_blank rel=noopener>Yang, Kevin, et al. &ldquo;Rlcd: Reinforcement learning from contrast distillation for language model alignment.&rdquo; arXiv preprint arXiv:2307.12950 (2023).</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Jan 14, 2025</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_12_13_turini_et_al_scientific_reports/>[Article] Hierarchical organization of objects in scenes is reflected in mental representations of objects</a></div><div class=mt-2><p>summary: This study explored how the hierarchical structure of scene grammar is reflected in our object recognition. Scenes are divided into several &ldquo;phrases,&rdquo; each consisting of a central &ldquo;anchor&rdquo; object and its surrounding &ldquo;local objects.&rdquo; Participants consistently judged object pairs within the same phrase to be more similar, and this tendency was observed across both images and words. This demonstrates that the hierarchical structure of the visual environment is integrated into our abstract mental representations. Consequently, this study is expected to provide insights into how stimuli can be extracted from natural scene data, such as COCO images.</p><p><a href=https://www.nature.com/articles/s41598-022-24505-x target=_blank rel=noopener>Turini, Jacopo, and Melissa Le-Hoa Võ. &ldquo;Hierarchical organization of objects in scenes is reflected in mental representations of objects.&rdquo; Scientific Reports 12.1 (2022): 20068.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Dec 13, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_11_27_ogedegbe_et_al_cardiology_clinics_lin_et_al_physiological_measurement_ding_et_al_scientific_reports/>[Article] Principles and Techniques of Blood Pressure Measurement & Investigating the physiological mechanisms of the photoplethysmogram features for blood pressure estimation & Pulse Transit Time Based Continuous Cuffless Blood Pressure Estimation: A New Extension and A Comprehensive Evaluation.</a></div><div class=mt-2><h3 id=principles-and-techniques-of-blood-pressure-measurement>Principles and Techniques of Blood Pressure Measurement</h3><p>Summary: The first study reviews the principles and techniques of blood pressure (BP) measurement, emphasizing the importance of accurate methods for clinical and research applications. It discusses various BP measurement techniques, including auscultatory, oscillometric, and ambulatory methods, and highlights potential sources of error and strategies to minimize them.</p><p><a href=https://www.cardiology.theclinics.com/article/S0733-8651%2810%2900086-X/abstract target=_blank rel=noopener>Ogedegbe, G., & Pickering, T. (2010). Principles and techniques of blood pressure measurement. Cardiology clinics, 28(4), 571-586.</a></p><h3 id=investigating-the-physiological-mechanisms-of-the-photoplethysmogram-features-for-blood-pressure-estimation>Investigating the physiological mechanisms of the photoplethysmogram features for blood pressure estimation</h3><p>Summary: The second study examines the physiological basis of PPG features for BP estimation by analyzing 65 features from 12 healthy subjects during cold stimuli and exercise recovery.</p><p><a href=https://iopscience.iop.org/article/10.1088/1361-6579/ab7d78/meta target=_blank rel=noopener>Lin, W. H., Li, X., Li, Y., Li, G., & Chen, F. (2020). Investigating the physiological mechanisms of the photoplethysmogram features for blood pressure estimation. Physiological measurement, 41(4), 044003.</a></p><h3 id=pulse-transit-time-based-continuous-cuffless-blood-pressure-estimation-a-new-extension-and-a-comprehensive-evaluation>Pulse Transit Time Based Continuous Cuffless Blood Pressure Estimation: A New Extension and A Comprehensive Evaluation</h3><p>Summary: The third study demonstrates a cuffless BP measurement method combining PTT and the novel PPG intensity ratio (PIR).</p><p><a href=https://www.nature.com/articles/s41598-017-11507-3/1000 target=_blank rel=noopener>Ding, X., Yan, B. P., Zhang, Y. T., Liu, J., Zhao, N., & Tsang, H. K. (2017). Pulse transit time based continuous cuffless blood pressure estimation: A new extension and a comprehensive evaluation. Scientific reports,
7(1), 1-11.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Nov 27, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_11_13_karakose-akbiyik_nature_communications_mcmahon_current_biology/>[Article] Hierarchical organization of social action features along the lateral visual pathway</a></div><div class=mt-2><p>summary: The first study investigates the hierarchical organization of social action features along the lateral visual stream, revealing that the brain processes increasingly complex features, from low-level motion in early visual areas to high-level communicative actions in the superior temporal sulcus (STS). The second study demonstrates a shared neural code for representing both human actions and object events, suggesting that the brain uses a common neural mechanism to interpret the physics of interactions, independent of animacy. Together, these findings provide new insights into the neural architecture underlying social perception and event understanding, highlighting both specialized and generalized processes in the human brain.</p><p><a href=//bspl.korea.ac.kr/Board/Members_Only/Lab_Seminar/KJE/Karakose-Akbiyik_et_al_2023.pdf>Karakose-Akbiyik, Seda, Alfonso Caramazza, and Moritz F. Wurm. &ldquo;A shared neural code for the physics of actions and object events.&rdquo; Nature Communications 14.1 (2023): 3316.</a></p><p><a href=//bspl.korea.ac.kr/Board/Members_Only/Lab_Seminar/KJE/McMahon_et_al_2023.pdf>McMahon, Emalie, Michael F. Bonner, and Leyla Isik. &ldquo;Hierarchical organization of social action features along the lateral visual pathway.&rdquo; Current Biology 33.23 (2023): 5035-5047.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Nov 13, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_10_30_singer_et_al_neuroimage/>[Article] Development and validation of an fMRI-informed EEG model of reward-related ventral striatum activation. Neuroimage.</a></div><div class=mt-2><p>summary: Here they develop and validate an accessible and affordable probe of neural activation related to reward processing in the ventral striatum(VS). Using an fMRI-informed EEG approach, they identified a particular spatial-temporal-spectral EEG representation that is predictive of the concurrently acquired fMRI activity in the ventral striatum while responding to rewarding stimuli. They found the VS-electrical fingerprint model to be correlated significantly with the BOLD signal in the VS and associated regions across individuals.</p><p><a href=https://www.sciencedirect.com/science/article/pii/S1053811923003348 target=_blank rel=noopener>Singer, Neomi, et al. &ldquo;Development and validation of an fMRI-informed EEG model of reward-related ventral striatum activation.&rdquo; Neuroimage 276 (2023): 120183.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Oct 30, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_09_11_giannakakis_et_al_ieee/>[Article] Review on Psychological Stress Detection Using Biosignals. IEEE Transactions on Affective Computing.</a></div><div class=mt-2><p>Summary: This paper provides a comprehensive overview of how psychological stress can be detected through various biosignals. It discusses the physiological processes triggered by stress, which are measurable through signals like EEG, ECG, EDA, and others(7 more bio-signals). The paper aims to establish reliable biosignal indices that can effectively indicate stress levels, emphasizing the need for consistency and robustness in biosignal data features.</p><p><a href=https://ieeexplore.ieee.org/abstract/document/8758154/ target=_blank rel=noopener>Giannakakis, Giorgos, et al. &ldquo;Review on psychological stress detection using biosignals.&rdquo; IEEE transactions on affective computing 13.1 (2019): 440-460.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Sep 11, 2024</span></div></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/seminar_papers/2024_08_29_cruz_et_al_ieee/>[Article] Eye-lrcn: A long-term recurrent convolutional network for eye blink completeness detection. IEEE Transactions on Neural Networks and Learning Systems.</a></div><div class=mt-2><p>Summary: The article introduces Eye-LRCN, a new method for eye blink detection that also evaluates blink completeness using a Long-Term Recurrent Convolutional Network (LRCN). This approach combines a CNN for feature extraction with a bidirectional RNN for sequence learning, and employs a Siamese architecture to handle class imbalance and limited data. Eye-LRCN demonstrates superior performance in blink detection and completeness assessment, and achieves noticeable results in eye state detection.</p><p><a href=https://ieeexplore.ieee.org/abstract/document/9885029 target=_blank rel=noopener>de la Cruz, Gonzalo, et al. &ldquo;Eye-lrcn: A long-term recurrent convolutional network for eye blink completeness detection.&rdquo; IEEE Transactions on Neural Networks and Learning Systems 35.4 (2022): 5130-5140.</a></p></div><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Aug 29, 2024</span></div></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/seminar_papers/>&#171;</a></li><li class=page-item><a class=page-link href=/seminar_papers/page/3/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2025 BSPL</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e762603fd04b1d81f1e953bafed73e89.js></script></body></html>